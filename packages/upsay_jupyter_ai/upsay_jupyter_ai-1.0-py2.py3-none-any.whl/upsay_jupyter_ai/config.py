from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, TextIteratorStreamer
from IPython.core.magic import register_line_magic, register_cell_magic, register_line_cell_magic
from IPython.display import display, Markdown, clear_output, HTML, update_display
from IPython import get_ipython
import torch
import pandas as pd
import ast
import numpy as np
from threading import Thread
import time
from rank_bm25 import BM25Okapi
import nltk
from nltk.tokenize import word_tokenize
from huggingface_hub import login
import random
import ipywidgets as widgets
import json

from langchain_openai import OpenAI, ChatOpenAI

# Necessary so that query embedding match (initialization) the pre-computed corpus embeddings
torch.manual_seed(0)
np.random.seed(0)

#########################################################################################################################################

class UPSayAI:
    def __init__(self, model_name="aristote/llama", model_access_token=None, model_temperature=0.8, model_max_tokens=2048, rag_model="dpr", rag_min_relevance=0.5, rag_max_results=5, recommendation_min_relevance=0.7, quiz_difficulty="moyen", quiz_min_relevance=0.2, quiz_max_results=100, num_questions_quiz=5, course_corpus_file="corpus_ISD.csv"):
        self.model_name = model_name
        self.model_access_token = model_access_token
        self.model_temperature = model_temperature
        self.model_max_tokens = int(model_max_tokens)
        self.rag_model = rag_model
        self.rag_min_relevance = rag_min_relevance
        self.rag_max_results = int(rag_max_results)
        self.recommendation_min_relevance = recommendation_min_relevance
        self.quiz_difficulty = quiz_difficulty
        self.quiz_min_relevance = quiz_min_relevance
        self.quiz_max_results = int(quiz_max_results)
        self.num_questions_quiz = int(num_questions_quiz)
        self.course_corpus_file = course_corpus_file


        self.check_values()
        
        self.config_rag()
        self.config_llm()

        self.setup_magics()

    # Check if the values are in the expected range of possible values
    def check_values(self):
        assert self.model_name in ["aristote/llama", "aristote/mixtral", "huggingface/llama"], "Invalid model_name. Current supported values are 'aristote/llama', 'aristote/mixtral', or 'huggingface/llama'"
        assert 0 <= self.model_temperature <= 1, "Invalid model_temperature. The temperature value has to be a float between 0 and 1"
        assert 512 <= self.model_max_tokens <= 4096, "Invalid model_max_tokens. The model_max_tokens value has to be an int between 512 and 4096"
        assert self.rag_model in ["dpr", "bm25"], "Invalid rag_model. Current supported values are 'dpr' or 'bm25'"
        assert 0 <= self.rag_min_relevance <= 1, "Invalid rag_min_relevance. The rag_min_relevance value has to be a float between 0 and 1"
        assert 1 <= self.rag_max_results <= 100, "Invalid rag_max_results. The rag_max_results value has to be an int between 1 and 100"
        assert 0 <= self.recommendation_min_relevance <= 1, "Invalid recommendation_min_relevance. The recommendation_min_relevance value has to be a float between 0 and 1"
        assert self.quiz_difficulty in ["facile", "moyen", "difficile"], "Invalid quiz_difficulte. Current supported values are 'facile', 'moyen', or 'difficile'"
        assert 0 <= self.quiz_min_relevance <= 1, "Invalid quiz_min_relevance. The quiz_min_relevance value has to be a float between 0 and 1"
        assert 10 <= self.quiz_max_results <= 1000, "Invalid quiz_max_results. The quiz_max_results value has to be an int between 10 and 1000"
        assert 3 <= self.num_questions_quiz <= 10, "Invalid num_questions_quiz. The num_questions_quiz value has to be an int between 3 and 10"


    # Config the RAG models and load 
    def config_rag(self):
        if self.rag_model == "dpr":
            self.load_corpus_embeddings()
            self.config_query_embedding()
        elif self.rag_model == "bm25":
            self.config_bm25()

    # Config the LLMs (using the config_llm_aristote or config_llm_huggingface functions) and define the persona prompts (role system)
    def config_llm(self):
        if self.model_name == "aristote/llama" or self.model_name == "aristote/mixtral":
            self.config_llm_aristote()
        elif self.model_name == "huggingface/llama":
            self.config_llm_huggingface()
            
        # Configure prompts for the LLMs
        
        # QR (for %ai_question)
        self.persona_prompt_qa = "Vous √™tes un assistant virtuel qui aide les √©tudiants de l'Universit√© Paris-Saclay avec des questions dans le domaine de la programmation et de la science des donn√©es, en r√©pondant toujours de mani√®re p√©dagogique et polie. Lorsque c'est possible, essayez d'utiliser des informations et des exemples tir√©s du mat√©riel de cours pour aider l'√©tudiant √† comprendre, en soulignant dans votre explication o√π l'√©tudiant a vu ce contenu √™tre employ√© pendant le cours et en mettant toujours en contexte pour une r√©ponse bien structur√©e. N'incluez pas d'images ou de m√©dias dans votre r√©ponse, uniquement du texte en format markdown."
        self.messages_history = []
        
        # Code generation (for %ai_code)
        self.persona_prompt_code = "Vous √™tes un assistant virtuel qui √©crit des codes python selon les instructions d'un √©tudiant de l'Universit√© Paris-Saclay. Utilisez toujours des commentaires et documentez bien vos codes pour les rendre faciles √† comprendre pour l'√©tudiant. Mettez toujours tout le code, y compris les √©ventuels exemples pratiques d'utilisation, dans un seul bloc d√©limit√© par '```python' au d√©but et '```' √† la fin. Ne g√©n√©rez pas plus d'un bloc de code, g√©n√©rez toujours un seul bloc avec tout le code et les exemples d'utilisation √† l'int√©rieur afin que l'√©tudiant puisse tout ex√©cuter dans une seule cellule jupyter. Utilisez des biblioth√®ques et des fonctions avec lesquelles l'√©tudiant est plus susceptible d'√™tre familier, donnez la pr√©f√©rence √† des solutions plus simples tant qu'elles sont correctes et enti√®rement fonctionnelles. Assurez-vous que votre code est correct, fonctionnel et que les √©ventuels exemples d'utilisation fonctionneront parfaitement et donneront des r√©sultats corrects lorsqu'ils seront ex√©cut√©s par l'√©tudiant. Terminez toujours par un court paragraphe apr√®s le bloc de code python (d√©limit√© par '```python' au d√©but et '```') avec une description textuelle et des explications pour l'√©tudiant afin d'am√©liorer sa compr√©hension du sujet et du code g√©n√©r√©."

        
        # Code explanation (for %ai_explain)
        self.persona_prompt_explain = "Vous √™tes un assistant virtuel qui explique des codes python √† un √©tudiant de l'Universit√© Paris-Saclay d'une mani√®re claire et informative. Fournissez une analyse textuelle compl√®te en fran√ßais du code que vous sera donn√©, en expliquant son objectif global, la logique utilis√©e, les principales variables, les biblioth√®ques et les fonctions utilis√©es par le programmeur, et les √©ventuels outputs attendus s'il y en a. Vous pouvez utiliser des extraits du code dans votre explication, d√©limit√© par '```python' au d√©but et '```' √† la fin, si vous pensez que cela aidera la compr√©hension de l'√©tudiant, et inclure des exemples possibles de quand et comment le code pourrait √™tre utilis√© par l'√©tudiant. Pr√©f√©rez expliquer l'objectif global et la logique dans un flux continu √† travers un paragraphe plut√¥t que d'utiliser des sous-titres, mais √©num√©rez les principales variables sous forme de puces. L'√©tudiant doit √™tre en mesure de comprendre pleinement le code apr√®s avoir lu votre explication. Terminez toujours par une conclusion r√©sumant l'explication. R√©digez toute votre explication en fran√ßais."


        # Code debugging (for %ai_debug)
        self.persona_prompt_debug = "Vous √™tes un assistant virtuel qui aide les √©tudiants de l'Universit√© Paris-Saclay √† d√©boguer des codes Python qui ne s'ex√©cutent pas. Analysez le code et le message d'erreur que vous seront donn√©s, en g√©n√©rant un rapport de d√©bogage avec une analyse textuelle expliquant les raisons qui causent les erreurs d'ex√©cution indiqu√©es par le message d'erreur. Enfin, proposez des solutions, d√©limit√©es par '```python' au d√©but et '```' √† la fin, que l'√©tudiant peut impl√©menter dans le code pour le corriger afin qu'il s'ex√©cute correctement sans erreurs."

        # Quiz (for %ai_quiz)
        # If student chooses a specific subject for the quiz (will be used in the instructions_prompt inside the ai_quiz function)
        self.quiz_subject = ""
        # The relevant cours content (cells from the notebooks) (is updated by the get_quiz_context function at every query)
        self.cours_content = ""
        # For prompt engineering, to help the LLM to understand the expected difficulty level
        if(self.quiz_difficulty == "facile"):
            self.quiz_level = "licence L1"
        elif(self.quiz_difficulty == "moyen"):
            self.quiz_level = "licence L3"
        elif(self.quiz_difficulty == "difficile"):
            self.quiz_level = "master M2"
        
        self.persona_prompt_quiz = f"Vous √™tes un g√©n√©rateur de quiz style QCM de niveau de difficult√© {self.quiz_difficulty} pour des √©tudiants de {self.quiz_level} qui souhaitent tester leurs connaissances dans le cadre de leurs √©tudes et de leur pr√©paration aux examens. Le quiz a {self.num_questions_quiz} questions en fran√ßais, o√π chaque question n'a qu'une seule r√©ponse correcte et trois r√©ponses incorrectes. Essayez de r√©diger les quatre choix de r√©ponses de longueur similaire pour √©viter que la bonne r√©ponse soit toujours la plus longue." + """ Organisez les questions g√©n√©r√©es dans une liste de dictionnaires python, o√π chaque dictionnaire repr√©sente une question format√©e comme suit : [{"√©nonc√©": "Question √† l'√©l√®ve (1) ?", "bonne_r√©ponse": "Ceci est la r√©ponse correcte √† la question 1.", "mauvaises_r√©ponses":["Ceci est la premi√®re r√©ponse incorrecte √† la question 1.", "Ceci est la deuxi√®me r√©ponse incorrecte √† la question 1.", "Ceci est la troisi√®me r√©ponse incorrecte √† la question 1."]}, {"√©nonc√©": "Question √† l'√©l√®ve (2) ?", "bonne_r√©ponse": "Ceci est la r√©ponse correcte √† la question 2.", "mauvaises_r√©ponses":["Ceci est la premi√®re r√©ponse incorrecte √† la question 2.", "Ceci est la deuxi√®me r√©ponse incorrecte √† la question 2.", "Ceci est la troisi√®me r√©ponse incorrecte √† la question 2."]}, ...].""" + f" Cours √† utiliser pour g√©n√©rer le quiz : {self.cours_content}"
        

    # Load corpus contents and embeddings from CSV file
    def load_corpus_embeddings(self):
        try:
            self.df = pd.read_csv(self.course_corpus_file)
        except:
            raise ValueError("Course corpus CSV file not found.")
        try:
            self.df['embeddings'] = self.df['embeddings'].apply(ast.literal_eval)
            self.df['embeddings'] = self.df['embeddings'].apply(lambda x: torch.from_numpy(np.array(x)))
            self.df = self.df[self.df["content"].apply(lambda x: isinstance(x, str))].reset_index(drop=True)
            self.corpus_embeddings = self.df["embeddings"].tolist()
            self.corpus_embeddings = torch.stack([doc for doc in self.corpus_embeddings])
            self.corpus_embeddings = self.corpus_embeddings.float()
        except:
            raise ValueError("Course corpus file does not contain expected columns")

    # Query embedding tokenizer and model from HuggingFace. Only called if rag_model="dpr". 
    def config_query_embedding(self):
        self.tokenizer_dpr = AutoTokenizer.from_pretrained("etalab-ia/dpr-question_encoder-fr_qa-camembert",  do_lower_case=True, resume_download=None)
        self.model_dpr = AutoModel.from_pretrained("etalab-ia/dpr-question_encoder-fr_qa-camembert", return_dict=True, resume_download=None)

    # Confif BM25 retrieval
    def config_bm25(self):
        try:
            self.df = pd.read_csv(self.course_corpus_file)
            self.df = self.df[self.df["content"].apply(lambda x: isinstance(x, str))].reset_index(drop=True)
        except:
            raise ValueError("Course corpus file file not found.")
        try:
            self.bm25_index = BM25Okapi(self.df.tokenized_content.tolist())
        except:
            raise ValueError("Course corpus file does not contain expected columns")
    
    # Config LLM for aristote models
    def config_llm_aristote(self):
        if self.model_name == "aristote/llama":
            self.model = ChatOpenAI(openai_api_base = "https://dispatcher.aristote.centralesupelec.fr/v1", openai_api_key = self.model_access_token, model = "casperhansen/llama-3-70b-instruct-awq", temperature=self.model_temperature, max_tokens=self.model_max_tokens)
        elif self.model_name == "aristote/mixtral":
            self.model = ChatOpenAI(openai_api_base = "https://dispatcher.aristote.centralesupelec.fr/v1", openai_api_key = self.model_access_token, model = "casperhansen/mixtral-instruct-awq", temperature=self.model_temperature, max_tokens=self.model_max_tokens)

    # Config LLM for huggingface models (currently only Meta-Llama-3-8B-Instruct)
    def config_llm_huggingface(self):
        login(token=self.model_access_token)
        model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    torch_dtype=torch.bfloat16
                )
        
        self.model.to(self.device)
        
        self.terminators = [
                        self.tokenizer.eos_token_id,
                        self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
                    ]
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        # Configure streamer for token-by-token display
        self.streamer = TextIteratorStreamer(self.tokenizer, skip_prompt = True)

    
        
    # Function to compute query embedding from the raw text (string)
    def get_query_embedding(self, text):
        input_ids = self.tokenizer_dpr(
            text, 
            return_tensors='pt',
            max_length = 512,
            truncation=True  # Truncate sequences longer than the maximum length
        )["input_ids"]
        embeddings = self.model_dpr.forward(input_ids).pooler_output
        return embeddings

    # Function to calculate cosine similarity between query embedding and every corpus cell embedding
    def cos_sim(self, a, b):
        a_norm = torch.nn.functional.normalize(a, p=2, dim=1)
        b_norm = torch.nn.functional.normalize(b, p=2, dim=1)
        return torch.mm(a_norm, b_norm.transpose(0, 1))

    # Function that generates the context to be passed as RAG to the LLM (context = concatenation of relevant cell contents)
    def get_context(self, query):
        if self.rag_model == "dpr":
            index_high_score = self.dpr_search(query, self.rag_min_relevance, self.rag_max_results)
        elif self.rag_model == "bm25":
            index_high_score = self.bm25_search(query)
        if(len(index_high_score) > 0):
            rag_content = "\n".join(self.df["content"].iloc[index_high_score])
            return rag_content
        return "Aucun extrait du support de cours n'a √©t√© trouv√© contenant des informations pertinentes sur la question pos√©e par l'√©tudiant. Si vous ne connaissez pas non plus la r√©ponse, informez l'√©tudiant que vous n'avez trouv√© aucune information sur ce qui lui est demand√© et recommandez-lui de contacter les professeurs responsables du cours."

    # Function that generates the reading recommendations (notebooks from the course Gitlab that are relevant and the student should review) as hyperlinks
    def get_recommendation(self, query_answer):
        if self.rag_model == "dpr":
            index_high_score = self.dpr_search(query_answer, self.recommendation_min_relevance, self.rag_max_results)
        elif self.rag_model == "bm25":
            index_high_score = self.bm25_search(query_answer)
        if(len(index_high_score) > 0):
            rag_courses =  self.df["file"].iloc[index_high_score].unique().tolist()
            rag_weeks =  self.df["folder"].iloc[index_high_score].unique().tolist()
            rag_recommendation = ", ".join([f"[`{rag_courses[i][:-3]}`](https://gitlab.dsi.universite-paris-saclay.fr/L1InfoInitiationScienceDonnees/Instructors/-/blob/main/{rag_weeks[i]}/{rag_courses[i]})" for i in range(len(rag_courses))])
            return rag_recommendation
        return None

    # Function that generates the context to be passed as RAG to the LLM (context = concatenation of relevant cell contents)
    def get_quiz_context(self, query):
        # For command with no argument
        if(query == ""):
            file_list = self.df.file.unique().tolist()
            file_list_filtered = [file for file in file_list if file != "CM1_1_presentation_UE.md"]
            notebook = random.choice(file_list_filtered)
            self.quiz_subject = ""
            return "\n".join(self.df[self.df["file"] == notebook]["content"].dropna())
        # For %ai_quiz CMx
        elif(query in self.df.file.unique().tolist()):
            self.quiz_subject = ""
            return "\n".join(self.df[self.df["file"] == query]["content"].dropna())
        # For %ai_quiz CMx.md
        elif(query in [x.replace(".md", "") for x in self.df.file.unique().tolist()]):
            self.quiz_subject = ""
            return "\n".join(self.df[self.df["file"] == query+".md"]["content"].dropna())
        # For %ai_quiz subject
        else:
            # The quiz for specific subject only works with dpr
            if self.rag_model != "dpr":
                return "error_rag"
            else:
                # For now, check if there are at least 3 cells with similarity above 0.35 to see if subject is related to the class
                check_subject_relevance = self.dpr_search(query, 0.35, 5)
                if(len(check_subject_relevance) > 3):
                    index_high_score = self.dpr_search(query, self.quiz_min_relevance, self.quiz_max_results)
                    self.quiz_subject = query
                    return "\n".join(self.df["content"].iloc[index_high_score])
                return None

    # IR function based on DPR to find relevant extracts
    def dpr_search(self, search_string, threshold, max_results):
        query_embedding = self.get_query_embedding(search_string)
        cos_scores = self.cos_sim(query_embedding, self.corpus_embeddings)[0]
        cos_scores_np = (-1) * np.sort(-cos_scores.detach().numpy())
        cos_idx_np = np.argsort(-cos_scores.detach().numpy())
        mask = cos_scores_np > threshold
        index_high_score = cos_idx_np[mask][:max_results]
        return index_high_score

    # IR function based on BM25 to find relevant extracts
    def bm25_search(self, search_string):
        search_tokens = word_tokenize(search_string)
        scores = self.bm25_index.get_scores(search_tokens)
        top_indexes = np.argsort(scores)[::-1][:self.rag_max_results]
        return top_indexes

    # LLM Generation function for aristote models
    def aristote_generate(self, messages, initial_response):
        generated_text = initial_response
        for chunk in self.model.stream(messages):
            generated_text += chunk.content
            update_display(Markdown(generated_text), display_id=self.display_output.display_id)
        return generated_text

    # LLM Generation function for huggingface models
    def huggingface_generate(self, messages, initial_response):
        # Preprocess the question
        input_ids = self.tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(self.device)
        
        generation_kwargs = {
            "input_ids": input_ids,
            "max_new_tokens": self.model_max_tokens,
            "eos_token_id": self.terminators,
            "pad_token_id": self.tokenizer.pad_token_id,
            "do_sample": True,
            "temperature": self.model_temperature,
            "top_p": 0.9,
            "streamer": self.streamer
        }
        # Generate answer and display it word by word as it is written (similar to ChatGPT user experience)
        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()
        generated_text = initial_response
        for new_text in self.streamer:
            generated_text += new_text.replace("<|eot_id|>", "")
            update_display(Markdown(generated_text), display_id=self.display_output.display_id)
        return generated_text

    # LLM Quiz Generation function for aristote models
    def generate_quiz_questions_aristote(self):
        chunks = ""
        for chunk in self.model.stream(self.quiz_messages):
            chunks += chunk.content
            if "{" in chunks and "}" in chunks:
                init = chunks.find("{")
                end = chunks.find("}") + 1
                dict_string = chunks[init:end]
                try:
                    true_dict = json.loads(dict_string)
                    answers = [true_dict["bonne_r√©ponse"]] + true_dict["mauvaises_r√©ponses"]
                    random.shuffle(answers)
                    true_dict["toutes_r√©ponses"] = answers
                    self.questions_llm.append(true_dict)
                except:
                    self.num_questions -= 1
                chunks = ""

    # LLM Quiz Generation function for huggingface models
    def generate_quiz_questions_huggingface(self):

        # Preprocess the question
        input_ids = self.tokenizer.apply_chat_template(
            self.quiz_messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(self.device)
        
        generation_kwargs = {
            "input_ids": input_ids,
            "max_new_tokens": 2048,
            "eos_token_id": self.terminators,
            "pad_token_id": self.tokenizer.pad_token_id,
            "do_sample": True,
            "temperature": self.model_temperature,
            "top_p": 0.9,
            "streamer": self.streamer
        }
        # Generate answer and display it word by word as it is written (similar to ChatGPT user experience)
        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
        thread.start()

        generated_text = ""
        for new_text in self.streamer:
            generated_text += new_text.replace("<|eot_id|>", "")
            if "{" in generated_text and "}" in generated_text:
                init = generated_text.find("{")
                end = generated_text.find("}") + 1
                dict_string = generated_text[init:end]
                try:
                    true_dict = json.loads(dict_string)
                    answers = [true_dict["bonne_r√©ponse"]] + true_dict["mauvaises_r√©ponses"]
                    random.shuffle(answers)
                    true_dict["toutes_r√©ponses"] = answers
                    self.questions_llm.append(true_dict)
                except:
                    self.num_questions -= 1
                generated_text = ""
        return generated_text

        
    # Displays the quiz interface
    def display_question(self, index):
        if index >= self.num_questions:
            self.finish_quiz()
            return
            
        with self.out:
            clear_output(wait=True)
            if len(self.questions_llm) <= index:
                display(HTML("<h4>‚è≥ Chargement de la question, veuillez patienter...</h4>"))
                while(len(self.questions_llm) <= index):
                    if(self.num_questions <= 0):
                        display(HTML("<h4>‚ùå La g√©n√©ration du quiz a √©chou√©. Veuillez relancer cette cellule.</h4>"))
                        return
                    elif index >= self.num_questions:
                        self.finish_quiz()
                        return
            question = self.questions_llm[index]["√©nonc√©"]
            right_answer = self.questions_llm[index]["bonne_r√©ponse"]
            answers = self.questions_llm[index]["toutes_r√©ponses"]
            clear_output(wait=True)
            display(Markdown(f"<h4>Question {index+1}/{self.num_questions}: {question}</h4>"))
            
            radio_buttons = widgets.RadioButtons(
                options=answers,
                value=self.selected_answers[index],  # Selected value
                description="Choisissez l'option correcte:",
                disabled=False,
                layout={'width': 'max-content'},
            )
    
            display(radio_buttons)
    
            if index > 0:
                button_back = widgets.Button(description='Pr√©c√©dent', disabled=False)
            else:
                button_back = widgets.Button(description='Pr√©c√©dent', disabled=True)
            button_back.style.button_color = 'lightblue'
            if index < (self.num_questions - 1):
                button_next = widgets.Button(description='Suivant')
            else:
                button_next = widgets.Button(description='Finaliser')
            button_next.style.button_color = 'lightgreen'  

            print("\n")
            button_box = widgets.HBox([button_back, button_next])
            display(button_box)
            
            def on_button_back_click(b):
                self.selected_answers[index] = radio_buttons.value
                self.display_question(index - 1)
    
            def on_button_next_click(b):
                self.selected_answers[index] = radio_buttons.value
                self.display_question(index + 1)
    
    
            button_back.on_click(on_button_back_click)
            button_next.on_click(on_button_next_click)
    
    # Function to finish quiz and show results
    def finish_quiz(self):
        final_answers = self.questions_llm.copy()
        number_correct = 0
        with self.out:
            clear_output(wait=True)
            display(HTML("<h2>Quiz termin√© !</h2>"))
            for i in range(len(final_answers)):
                final_answers[i]["r√©ponse_s√©lectionn√©e"] = self.selected_answers[i]
                if final_answers[i]["r√©ponse_s√©lectionn√©e"] == final_answers[i]["bonne_r√©ponse"]:
                    number_correct += 1
                    display(HTML(f"<h4>‚úîÔ∏è Question {i+1} : {final_answers[i]['√©nonc√©']}</h4>"))
                    display(Markdown(f"**R√©ponse S√©lectionn√©e :** {final_answers[i]['r√©ponse_s√©lectionn√©e']}"))
                    print("\n")
                else:
                    display(HTML(f"<h4>‚ùå Question {i+1} : {final_answers[i]['√©nonc√©']}</h4>"))
                    display(Markdown(f"**R√©ponse S√©lectionn√©e :** {final_answers[i]['r√©ponse_s√©lectionn√©e']}"))
                    display(Markdown(f"**R√©ponse Correcte :** {final_answers[i]['bonne_r√©ponse']}"))
                    print("\n")
            display(HTML(f"<h4>üìú Votre nombre de r√©ponses correctes est de {number_correct}/{self.num_questions}.</h4>"))
            print("\n")
            button_feedback = widgets.Button(description='Feedback')
            button_feedback.style.button_color = 'lightgreen' 
            display(button_feedback)
            def on_button_feedback_click(b):
                self.out.clear_output()
                with self.out:
                    prompt_feedback = f"G√©n√©rez un feedback pour un √©tudiant √† partir des r√©ponses qu'il a donn√©es √† un quiz de type QCM. Organisez votre feedback en rappelant toujours les questions et les r√©ponses lorsque l'√©tudiant n'a plus acc√®s au quiz, en expliquant pourquoi la bonne alternative est correcte et pourquoi les mauvaises alternatives sont incorrectes. Si l'√©l√®ve a choisi l'alternative incorrecte, renforcez votre explication sur la source possible de la confusion qui l'a conduit √† se tromper dans la question et soulignez, parmi les alternatives disponibles, celle qui est correcte. R√©capitulatif du quiz : {final_answers}"
                    chunks = "# üìö Feedback sur le quiz : \n\n"
                    if self.model_name == "aristote/llama" or self.model_name == "aristote/mixtral":
                        generated_text = self.aristote_generate(prompt_feedback, chunks)
                    else:
                        messages = [{"role": "user", "content": prompt_feedback}]
                        generated_text = self.huggingface_generate(messages, chunks)
            
            button_feedback.on_click(on_button_feedback_click)



    # Config the magic commands and display initial message
    def setup_magics(self):
        self.ip = get_ipython()
        if self.ip:
            self.ip.register_magic_function(self.ai_question, 'line_cell', 'ai_question')
            self.ip.register_magic_function(self.ai_code, 'line_cell', 'ai_code')
            self.ip.register_magic_function(self.ai_explain, 'line_cell', 'ai_explain')
            self.ip.register_magic_function(self.ai_debug, 'line_cell', 'ai_debug')
            self.ip.register_magic_function(self.ai_quiz, 'line', 'ai_quiz')
            self.ip.register_magic_function(self.ai_help, 'line', 'ai_help')
            clear_output(wait=True)
            display(Markdown("**UPSay AI Magic Commands** charg√© avec succ√®s ‚úÖ"))
            display(Markdown("Cette bo√Æte √† outils est exp√©rimentale et en phase de test. Elle peut donc pr√©senter des anomalies."))
            display(Markdown("Utilisez `%ai_help` pour acc√©der au Guide d'Utilisation."))

    # %ai_question magic command
    def ai_question(self, line="", cell=""):
        question = line + cell
        rag = self.get_context(question)
        rag_prompt = f" Voici un extrait du support de cours qui pourra vous √™tre utile dans votre r√©ponse √† l'√©tudiant : {rag}"
        # Append the users's current question to the messages history
        self.messages_history.append({"role": "user", "content": question})
        # The messages history does not include the RAG results for every exchange. Only the RAG of the current question is present in the messages passed to the LLM
        if self.model_name == "aristote/mixtral":
            messages = [{"role": "user", "content": self.persona_prompt_qa + rag_prompt}, {"role": "assistant", "content": "Bien s√ªr, je serais ravi de vous aider avec vos questions sur la programmation et la science des donn√©es."}] + [x for x in self.messages_history]
        else:
            messages = [{"role": "system", "content": self.persona_prompt_qa + rag_prompt}] + [x for x in self.messages_history]
        initial_response = ""
        clear_output(wait=True)
        self.display_output = display(Markdown(initial_response), display_id=True)
        # Generate answer and display it word by word as it is written (similar to ChatGPT user experience)
        if self.model_name == "aristote/llama" or self.model_name == "aristote/mixtral":
            generated_text = self.aristote_generate(messages, initial_response)
        elif self.model_name == "huggingface/llama":
            generated_text = self.huggingface_generate(messages, initial_response)


        # Append the assistant's answer to the messages history
        self.messages_history.append({"role": "assistant", "content": generated_text})
        # Use the assistant's answer alongside the question to find possible recommendations for the student 
        recommendation = self.get_recommendation(question + " " + generated_text)
        if(recommendation):
            generated_text += "\n\n"
            recommendation_string = "üí° Pour plus d'informations sur ce sujet, il peut √™tre utile de cliquer sur les liens pour r√©viser les cours :"
            # Also display the recommendation word by word to improve user experience (seamlessly integration with LLM answer)
            for word in recommendation_string.split():
                generated_text += word + ' '
                #display(Markdown(generated_text), clear = True)
                update_display(Markdown(generated_text), display_id=self.display_output.display_id)
                time.sleep(0.05)
            generated_text += recommendation
            #update_display(Markdown(f"{generated_text}{recommendation}."), display_id=self.display_output.display_id)
            update_display(Markdown(generated_text), display_id=self.display_output.display_id)
        
        warning_ia = "\n\n‚ö†Ô∏è R√©ponse g√©n√©r√©e par une intelligence artificielle, les informations peuvent ne pas √™tre exactes."
        generated_text += warning_ia
        update_display(Markdown(generated_text), display_id=self.display_output.display_id)
            
        # Limit history length to 10 exchanges user-assistant (i.e. max = 20 messages). If it passes this limit, delete first (oldest) exchange.
        if(len(self.messages_history)>20):
            self.messages_history.pop(0)
            self.messages_history.pop(0)

    # %ai_code magic command
    def ai_code(self, line="", cell=""):
        code_inst = line
        if cell != "":
            code_inst += "\n" + cell
        # No RAG for now
        if self.model_name == "aristote/mixtral":
            messages = [{"role": "user", "content": self.persona_prompt_code}, {"role": "assistant", "content": "Bien s√ªr, je serais ravi de vous aider avec vos codes python."},  {"role": "user", "content": code_inst}]
        else:
            messages = [{"role": "system", "content": self.persona_prompt_code}, {"role": "user", "content": code_inst}]
        initial_response = "‚è≥ Une fois la r√©ponse compl√©t√©e, le code sera d√©plac√© dans une nouvelle cellule juste en dessous.\n\n"
        clear_output(wait=True)
        self.display_output = display(Markdown(initial_response), display_id=True)
        
        if self.model_name == "aristote/llama" or self.model_name == "aristote/mixtral":
            generated_text = self.aristote_generate(messages, initial_response)
        elif self.model_name == "huggingface/llama":
            generated_text = self.huggingface_generate(messages, initial_response)

        generated_text = generated_text.replace(initial_response, "") # Delete the initial (loading) message
        # Check for the code block inside of the generated answer
        if '```python' in generated_text and '```\n' in generated_text:
            code_init = generated_text.find('```python') + len('```python')
            code_end = generated_text.find('```\n')
            # Put a comment before
            if(cell == ""):
                code = "# ‚ö†Ô∏è Code g√©n√©r√© par une intelligence artificielle sujet √† des erreurs\n" + f"# Instructions pour la g√©n√©ration : '{code_inst}'\n" + generated_text[code_init:code_end]
            else:
                code = "# ‚ö†Ô∏è Code g√©n√©r√© par une intelligence artificielle sujet √† des erreurs\n" + f"'''\nInstructions pour la g√©n√©ration : \n{code_inst}'''\n" + generated_text[code_init:code_end]
            code_remove = generated_text[(code_init - len('```python')):(code_end + len('```\n'))]
            generated_text = generated_text.replace(code_remove, "")
            generated_text += "\n\n" + "Le code g√©n√©r√© par l'IA a √©t√© ins√©r√© dans la cellule ci-dessous ‚¨áÔ∏è"
            self.ip.set_next_input(code) # Generate new cell with code
        display(Markdown(generated_text), clear = True)


    def ai_explain(self, line="", cell=""):
        initial_response = ""
        try:
            # For the "%ai_explain XX" format
            number_cell = int(line.strip())
            cell_content = self.ip.user_ns['In'][number_cell]
            # Since {} are seen as placeholders in f-strings and cause errors, we change them for {{}}
            clean_cell = cell_content.replace('{', '{{').replace('}', '}}')
        except:
            # For when the user puts the command on the first line of the code cell itself
            # Since {} are seen as placeholders in f-strings and cause errors, we change them for {{}}
            clean_cell = cell.replace('{', '{{').replace('}', '}}')
            
            initial_response = "‚ö†Ô∏è Note : Afin d'ex√©cuter le contenu de cette cellule plut√¥t que d'en g√©n√©rer une explication, vous devez supprimer la commande magique `%%ai_explain` sur la premi√®re ligne et ex√©cuter cette cellule √† nouveau.\n\n"
            
        if self.model_name == "aristote/mixtral":
            messages = [{"role": "user", "content": self.persona_prompt_explain}, {"role": "assistant", "content": "Bien s√ªr, je g√©n√©rerai une explication sur le code donn√© !"}, {"role": "user", "content": clean_cell}]
        else:
            messages = [{"role": "system", "content": self.persona_prompt_explain}, {"role": "user", "content": clean_cell}]
            
        clear_output(wait=True)
        self.display_output = display(Markdown(initial_response), display_id=True)
        
        if self.model_name == "aristote/llama" or self.model_name == "aristote/mixtral":
            generated_text = self.aristote_generate(messages, initial_response)
        elif self.model_name == "huggingface/llama":
            generated_text = self.huggingface_generate(messages, initial_response)


    def ai_debug(self, line="", cell=""):
        initial_response = ""
        try:
            # For the "%ai_explain XX" format
            number_cell = int(line.strip())
            cell_content = self.ip.user_ns['In'][number_cell]
            # Since {} are seen as placeholders in f-strings and cause errors, we change them for {{}}
            clean_cell = cell_content.replace('{', '{{').replace('}', '}}')
            cell_out = self.ip.run_cell(self.ip.user_ns['In'][number_cell])
            clear_output(wait=True)
            if not cell_out.success:
                error_message = f"error_before_exec='{cell_out.error_before_exec}', error_in_exec='{cell_out.error_in_exec}'"
            else:
                display(HTML("<h4>‚úÖ Le code fourni ne pr√©sente aucun message d'erreur !</h4>"))
                display(Markdown("Si vous avez des doutes sur la logique utilis√©e dans le code ou sur les r√©sultats g√©n√©r√©s, vous pouvez utiliser la commande `%%ai_question` pour poser des questions sp√©cifiques."))
        except:
            # For when the user puts the command on the first line of the code cell itself
            # Since {} are seen as placeholders in f-strings and cause errors, we change them for {{}}
            clean_cell = cell.replace('{', '{{').replace('}', '}}')
            cell_out = self.ip.run_cell(cell)
            clear_output(wait=True)
            if not cell_out.success:
                error_message = f"error_before_exec='{cell_out.error_before_exec}', error_in_exec='{cell_out.error_in_exec}'"
                initial_response = "‚ö†Ô∏è Note : Apr√®s avoir corrig√© le bug, afin de bien ex√©cuter le contenu de cette cellule plut√¥t que d'en g√©n√©rer un rapport de d√©bogage, vous devez supprimer la commande magique `%%ai_debug` sur la premi√®re ligne.\n\n"
            else:
                display(HTML("<h4>‚úÖ Le code fourni ne pr√©sente aucun message d'erreur !</h4>"))
                display(Markdown("Si vous avez des doutes sur la logique utilis√©e dans le code ou sur les r√©sultats g√©n√©r√©s, vous pouvez utiliser la commande `%%ai_question` pour poser des questions sp√©cifiques."))
                display(Markdown("Pour bien ex√©cuter le contenu de cette cellule, vous devez supprimer la commande magique `%%ai_debug` sur la premi√®re ligne."))
                
        if not cell_out.success:
            if self.model_name == "aristote/mixtral":
                messages = [{"role": "user", "content": self.persona_prompt_debug}, {"role": "assistant", "content": "Bien s√ªr, je g√©n√©rerai un un rapport de d√©bogage sur le code donn√© !"}, {"role": "user", "content": f"Code :\n{clean_cell}\nMessage d'erreur :\n{error_message}"}]
            else:
                messages = [{"role": "system", "content": self.persona_prompt_debug}, {"role": "user", "content": f"Code :\n{clean_cell}\nMessage d'erreur :\n{error_message}"}]
                
            clear_output(wait=True)
            self.display_output = display(Markdown(initial_response), display_id=True)
            
            if self.model_name == "aristote/llama" or self.model_name == "aristote/mixtral":
                generated_text = self.aristote_generate(messages, initial_response)
            elif self.model_name == "huggingface/llama":
                generated_text = self.huggingface_generate(messages, initial_response)

    # %ai_question magic command
    def ai_quiz(self, line=""):
        self.selected_answers = [None, None, None, None, None]
        self.num_questions = self.num_questions_quiz
        self.cours_content = self.get_quiz_context(line.strip())
        clear_output(wait=True)
        if(self.cours_content == None):
            display(HTML("<h5>‚ö†Ô∏è Je n'ai pas pu trouver de liens entre le sujet saisi et le mat√©riel de cours. Essayez de reformuler le sujet ou de demander un quiz sur un autre th√®me.</h5>"))
        elif(self.cours_content == "error_rag"):
            display(HTML("<h5>‚ö†Ô∏è Pour g√©n√©rer un quiz sur un sujet sp√©cifique, utilisez rag_model = 'dpr'.</h5>"))
        else:
            if(self.quiz_subject != ""):
                instructions_prompt = f"G√©n√©rez {self.num_questions_quiz} questions en fran√ßais style QCM de niveau de difficult√© {self.quiz_difficulty} (niveau {self.quiz_level} √† l'universit√©) sur le cours de {self.quiz_subject} donn√©. Basez vos questions principalement sur les contenus li√©s √† {self.quiz_subject} couverts par le cours susceptibles d'√™tre abord√©s lors d'examens futurs. N'h√©sitez pas √† poser des questions sur {self.quiz_subject} concernant des d√©tails qui n'apparaissent pas explicitement dans le cours mais que l'√©tudiant devrait conna√Ætre, √† condition qu'elles soient pertinentes et au m√™me niveau de difficult√©. N'oubliez pas de bien s√©parer les √©l√©ments des dictionnaires et de la liste par des virgules conform√©ment aux instructions donn√©es et aux standards python. Ne retournez rien d'autre que la liste des dictionnaires python dans le bon format avec les questions QCM en fran√ßais sur le sujet {self.quiz_subject}."
            else:
                instructions_prompt = f"G√©n√©rez {self.num_questions_quiz} questions en fran√ßais style QCM de niveau de difficult√© {self.quiz_difficulty} (niveau {self.quiz_level} √† l'universit√©) sur le cours donn√©. Basez vos questions principalement sur les contenus li√©s √† l'Introduction √† la Science des Donn√©es (statistiques, probabilit√©s, math√©matiques, programmation python, apprentissage statistique, etc) couverts par le cours susceptibles d'√™tre abord√©s lors d'examens futurs. N'h√©sitez pas √† poser des questions sur le m√™me sujet concernant des d√©tails qui n'apparaissent pas explicitement dans le cours mais que l'√©tudiant devrait conna√Ætre, √† condition qu'elles soient pertinentes et au m√™me niveau de difficult√©. N'oubliez pas de bien s√©parer les √©l√©ments des dictionnaires et de la liste par des virgules conform√©ment aux instructions donn√©es et aux standards python. Ne retournez rien d'autre que la liste des dictionnaires python dans le bon format avec les questions QCM en fran√ßais sur le cours donn√©."
            
            if self.model_name == "aristote/mixtral":
                self.quiz_messages = [{"role": "user", "content": self.persona_prompt_quiz}, {"role": "assistant", "content": "Bien s√ªr, je g√©n√©rerai le quiz sur le cours donn√© !"}, {"role": "user", "content": instructions_prompt}]
            else:
                self.quiz_messages = [{"role": "system", "content": self.persona_prompt_quiz}, {"role": "user", "content": instructions_prompt}]
            
            self.questions_llm = []
            if self.model_name == "aristote/llama" or self.model_name == "aristote/mixtral":
                Thread(target=self.generate_quiz_questions_aristote).start()
                
            elif self.model_name == "huggingface/llama":
                clear_output(wait=True)
                display(HTML("<h4>‚è≥ Chargement des questions, veuillez patienter...</h4>"))
                display(HTML("<h5>Les mod√®les Hugging Face ne permettent pas encore d'afficher les questions d√®s qu'elles sont pr√™tes, il faut donc attendre que toutes les questions soient g√©n√©r√©es.</h5>"))
                generated_text = self.generate_quiz_questions_huggingface()
                clear_output(wait=True)
                
            self.display_output = display(Markdown(""), display_id=True)
            self.out = widgets.Output()
            display(self.out)
            # Start the interactive loop
            self.display_question(0)




    
    
    # %ai_help magic command
    def ai_help(self, line):
        display(HTML("<h2>Guide d'Utilisation</h2>"))
        display(Markdown("L'**UPSay AI Magic Commands** est un ensemble d'outils exp√©rimentaux d'IA G√©n√©rative inspir√© de jupyter-ai et actuellement en cours de d√©veloppement √† l'Universit√© Paris-Saclay."))
        display(Markdown("<h4>Liste des Commandes Magiques :</h4>"))
        display(HTML("<hr>"))
        display(HTML("<h4><strong><code>%ai_question</code></strong></h4>"))
        display(Markdown("**Description :** Commande magique con√ßue pour les questions/r√©ponses (Q&R)."))
        display(Markdown("**Mode d'emploi :** Placez la commande dans la premi√®re ligne d'une cellule, suivie d'une question sur la m√™me ligne. La r√©ponse appara√Ætra au format Markdown juste en dessous de la cellule."))
        display(Markdown("**Exemple d'utilisation :**"))
        display(Markdown(f"<span style='color: gray;'>[ 42 ] : </span>`%ai_question Quelle est la diff√©rence entre causalit√© et corr√©lation ?{' '.join([' ' for i in range(200)])}`"))
        display(Markdown("**Note :** Vous pouvez utiliser **`%%ai_question`** (double %) si vous pr√©f√©rez utiliser plusieurs lignes pour formuler votre question."))
        display(HTML("<hr>"))
        display(HTML("<h4><strong><code>%ai_code</code></strong></h4>"))
        display(Markdown("**Description :** Commande magique con√ßue pour la g√©n√©ration de code (python)."))
        display(Markdown("**Mode d'emploi :** Placez la commande dans la premi√®re ligne d'une cellule, suivie des instructions pour la g√©n√©ration du code sur la m√™me ligne. Le code g√©n√©r√© appara√Ætra dans une nouvelle cellule."))
        display(Markdown("**Exemple d'utilisation :**"))
        display(Markdown(f"<span style='color: gray;'>[ 42 ] : </span>`%ai_code Fonction qui calcule le d√©terminant d'une matrice numpy.{' '.join([' ' for i in range(200)])}`"))
        display(Markdown("**Note :** Vous pouvez utiliser **`%%ai_code`** (double %) si vous pr√©f√©rez utiliser plusieurs lignes pour formuler vos intructions de code."))
        display(HTML("<hr>"))
        display(HTML("<h4><strong><code>%ai_explain</code></strong></h4>"))
        display(Markdown("**Description :** Commande magique con√ßue pour les explications de code."))
        display(Markdown("**Mode d'emploi :** Il existe deux mani√®res d'utiliser cette commande :"))
        display(Markdown("‚Ä¢ Placez la commande `%ai_explain` dans la premi√®re ligne d'une cellule, suivie sur la m√™me ligne du num√©ro de la cellule o√π se trouve le code √† expliquer (il faut qu'elle ait √©t√© ex√©cut√©e dans la session en cours)."))
        display(Markdown("‚Ä¢ Placez la commande `%%ai_explain` (double %) dans la premi√®re ligne de la cellule o√π se trouve le code √† expliquer. Le code de la cellule ne sera pas ex√©cut√©, seule son explication sera g√©n√©r√©e."))
        display(Markdown("**Exemple d'utilisation :** (Supposons que le code √† expliquer ait √©t√© ex√©cut√© dans la cellule [ 21 ] pendant la session en cours)"))
        display(Markdown(f"<span style='color: gray;'>[ 42 ] : </span>`%ai_explain 21{' '.join([' ' for i in range(200)])}`"))
        display(HTML("<hr>"))
        display(HTML("<h4><strong><code>%ai_debug</code></strong></h4>"))
        display(Markdown("**Description :** Commande magique con√ßue pour le d√©bogage de code."))
        display(Markdown("**Mode d'emploi :** Il existe deux mani√®res d'utiliser cette commande :"))
        display(Markdown("‚Ä¢ Placez la commande `%ai_debug` dans la premi√®re ligne d'une cellule, suivie sur la m√™me ligne du num√©ro de la cellule o√π se trouve le code √† d√©boguer (il faut qu'elle ait √©t√© ex√©cut√©e dans la session en cours)."))
        display(Markdown("‚Ä¢ Placez la commande `%%ai_debug` (double %) dans la premi√®re ligne de la cellule o√π se trouve le code √† d√©boguer. Le code de la cellule ne sera pas ex√©cut√©, seule son analyse de d√©bogage sera g√©n√©r√©e."))
        display(Markdown("**Exemple d'utilisation :** (Supposons que le code √† d√©boguer ait √©t√© ex√©cut√© dans la cellule [ 21 ] pendant la session en cours)"))
        display(Markdown(f"<span style='color: gray;'>[ 42 ] : </span>`%ai_debug 21{' '.join([' ' for i in range(200)])}`"))
        display(Markdown("**Note :** Uniquement pour les codes qui pr√©sentent un message d'erreur lorsqu'ils sont ex√©cut√©s. Pour les codes qui s'ex√©cutent mais g√©n√®rent des r√©sultats inattendus, vous pouvez utiliser la commande `%%ai_question` pour poser des questions sp√©cifiques."))
        display(HTML("<hr>"))
        display(HTML("<h4><strong><code>%ai_quiz</code></strong></h4>"))
        display(Markdown("**Description :** Commande magique con√ßue pour la g√©n√©ration de quiz style QCM."))
        display(Markdown("**Mode d'emploi :** Il existe trois mani√®res d'utiliser cette commande, en fonction du sujet du quiz que vous souhaitez obtenir :"))
        display(Markdown("‚Ä¢ Placez la commande toute seule dans la premi√®re ligne d'une cellule vide : pour g√©n√©rer un quiz sur une le√ßon (notebook du cours) al√©atoire."))
        display(Markdown("‚Ä¢ Placez la commande dans la premi√®re ligne d'une cellule, suivie du sujet souhait√© pour le quiz sur la m√™me ligne : pour g√©n√©rer un quiz sur un sujet sp√©cifique."))
        display(Markdown("‚Ä¢ Placez la commande dans la premi√®re ligne d'une cellule, suivie du nom d'une le√ßon (notebook du cours) au format CMx ou CMx.md sur la m√™me ligne : pour g√©n√©rer un quiz sur une le√ßon sp√©cifique."))
        display(Markdown("**Exemples d'utilisation :**"))
        display(Markdown(f"<span style='color: gray;'>[ 42 ] : </span>`%ai_quiz{' '.join([' ' for i in range(200)])}`"))
        display(Markdown(f"<span style='color: gray;'>[ 42 ] : </span>`%ai_quiz r√©seau de neurones{' '.join([' ' for i in range(200)])}`"))
        display(Markdown(f"<span style='color: gray;'>[ 42 ] : </span>`%ai_quiz CM4{' '.join([' ' for i in range(200)])}`"))
        display(HTML("<hr>"))
        
